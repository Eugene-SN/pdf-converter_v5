#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô DAG: Document Preprocessing - –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å Bridge-—Ñ–∞–π–ª–∞–º–∏

–ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –í –í–ï–†–°–ò–ò 4.0:
- ‚úÖ –†–µ—à–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ TableData —á–µ—Ä–µ–∑ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å document-processor API
- ‚úÖ –£–±—Ä–∞–Ω—ã –ø—Ä—è–º—ã–µ –∏–º–ø–æ—Ä—Ç—ã Docling –∏–∑ Airflow (–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–æ–≤)
- ‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ master_run_id (–∞–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –∏–∑ context)
- ‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ Permission denied —Å /app/temp (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ $AIRFLOW_HOME/temp)
- ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω retry –ø—Ä–∏ –æ—à–∏–±–∫–µ —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü (extract_tables=False)
- ‚úÖ –ù–û–í–û–ï: –í—ã–±–æ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ doc_*_intermediate.json –∏–∑ output_files
- ‚úÖ –ù–û–í–û–ï: –ó–∞–º–µ–Ω–∞ –ø—Ä–µ—Ñ–∏–∫—Å–∞ –ø—É—Ç–∏ /app/temp ‚Üí /opt/airflow/temp  
- ‚úÖ –ù–û–í–û–ï: –°–æ–∑–¥–∞–Ω–∏–µ bridge-—Ñ–∞–π–ª–∞ stage1_bridge_{timestamp}.json
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.exceptions import AirflowException
import os
import json
import logging
import time
import requests
from typing import Dict, Any, Optional, Tuple

# –ò–º–ø–æ—Ä—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —É—Ç–∏–ª–∏—Ç
from shared_utils import (
    SharedUtils, NotificationUtils, ConfigUtils,
    MetricsUtils, ErrorHandlingUtils
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è DAG
DEFAULT_ARGS = {
    'owner': 'pdf-converter',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=3),
}

dag = DAG(
    'document_preprocessing',
    default_args=DEFAULT_ARGS,
    description='DAG 1: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ PDF –≤ Markdown —á–µ—Ä–µ–∑ document-processor API',
    schedule_interval=None,
    max_active_runs=3,
    catchup=False,
    tags=['pdf-converter', 'dag1', 'chinese-docs', 'production', 'v4.0-fixed']
)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤
DOCUMENT_PROCESSOR_URL = os.getenv('DOCUMENT_PROCESSOR_URL', 'http://document-processor:8001')

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
CHINESE_DOC_CONFIG = {
    'ocr_languages': 'chi_sim,chi_tra,eng',
    'ocr_confidence_threshold': 0.75,
    'chinese_header_patterns': [
        r'^[Á¨¨Á´†ËäÇ]\s*[‰∏Ä‰∫å‰∏âÂõõ‰∫îÂÖ≠‰∏ÉÂÖ´‰πùÂçÅ\d]+\s*[Á´†ËäÇ]',
        r'^[‰∏Ä‰∫å‰∏âÂõõ‰∫îÂÖ≠‰∏ÉÂÖ´‰πùÂçÅ]+[„ÄÅÔºé]',
        r'^\d+[„ÄÅÔºé]\s*[\u4e00-\u9fff]',
    ],
    'tech_terms': {
        'ÈóÆÂ§©': 'WenTian',
        'ËÅîÊÉ≥ÈóÆÂ§©': 'Lenovo WenTian',
        'Â§©Êìé': 'ThinkSystem',
        'Ëá≥Âº∫': 'Xeon',
        'ÂèØÊâ©Â±ïÂ§ÑÁêÜÂô®': 'Scalable Processors',
        'Ëã±ÁâπÂ∞î': 'Intel',
        'Â§ÑÁêÜÂô®': 'Processor',
        'ÂÜÖÊ†∏': 'Core',
        'Á∫øÁ®ã': 'Thread',
        'ÂÜÖÂ≠ò': 'Memory',
        'Â≠òÂÇ®': 'Storage',
        '‰ª•Â§™ÁΩë': 'Ethernet',
        'Êú∫Êû∂': 'Rack',
        'ÊèíÊßΩ': 'Slot',
        'ÁîµÊ∫ê': 'Power Supply'
    }
}

def validate_input_file(**context) -> Dict[str, Any]:
    """‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å –∞–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ–º master_run_id"""
    start_time = time.time()
    try:
        dag_run_conf = context['dag_run'].conf or {}
        logger.info(f"üìã –ü–æ–ª—É—á–µ–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {json.dumps(dag_run_conf, indent=2, ensure_ascii=False)}")

        # –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (master_run_id —É–±—Ä–∞–Ω –∏–∑ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö!)
        required_params = ['input_file', 'filename', 'timestamp']
        missing_params = [param for param in required_params if not dag_run_conf.get(param)]
        if missing_params:
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {missing_params}")

        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ê–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ master_run_id –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏
        master_run_id = dag_run_conf.get('master_run_id') or context['dag_run'].run_id
        if 'master_run_id' not in dag_run_conf:
            logger.info(f"üÜî master_run_id –∞–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω –∏–∑ context: {master_run_id}")
            dag_run_conf['master_run_id'] = master_run_id

        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞
        input_file = dag_run_conf['input_file']
        # –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø: —Ö–æ—Å—Ç–æ–≤—ã–π –ø—Ä–µ—Ñ–∏–∫—Å -> –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–Ω—ã–π
        if isinstance(input_file, str) and input_file.startswith('/mnt/storage/apps/pdf-converter/'):
            input_file = input_file.replace('/mnt/storage/apps/pdf-converter/', '/app/', 1)
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –ø—É—Ç—å –≤ –∫–æ–Ω—Ñ–∏–≥
        dag_run_conf['input_file'] = input_file

        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞ –ø–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–Ω–æ–º—É –ø—É—Ç–∏
        if not SharedUtils.validate_input_file(input_file):
            raise ValueError(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–∞–π–ª: {input_file}")

        # –ê–Ω–∞–ª–∏–∑ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        file_info = analyze_chinese_document(input_file)

        # –û–±–æ–≥–∞—â–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        enriched_config = {
            **dag_run_conf,
            **file_info,
            'chinese_doc_analysis': file_info,
            'processing_mode': 'chinese_optimized',
            'validation_timestamp': datetime.now().isoformat()
        }

        MetricsUtils.record_processing_metrics(
            dag_id='document_preprocessing',
            task_id='validate_input_file',
            processing_time=time.time() - start_time,
            success=True
        )

        logger.info(f"‚úÖ –í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω: {dag_run_conf['filename']}")
        return enriched_config

    except Exception as e:
        MetricsUtils.record_processing_metrics(
            dag_id='document_preprocessing',
            task_id='validate_input_file',
            processing_time=time.time() - start_time,
            success=False
        )
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
        raise

def analyze_chinese_document(file_path: str) -> Dict[str, Any]:
    """–ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        file_size = os.path.getsize(file_path)
        file_hash = SharedUtils.calculate_file_hash(file_path)

        # –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
        estimated_pages = max(1, file_size // 102400)  # ~100KB –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
        has_chinese_text = False

        # –ü–æ–ø—ã—Ç–∫–∞ –±—ã—Å—Ç—Ä–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —á–µ—Ä–µ–∑ PyMuPDF –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        try:
            import fitz
            doc = fitz.open(file_path)
            estimated_pages = doc.page_count

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç
            for page_num in range(min(2, doc.page_count)):
                page = doc[page_num]
                text = page.get_text()[:500]  # –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤
                chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
                if chinese_chars > 5:
                    has_chinese_text = True
                    break
            doc.close()
        except Exception:
            pass  # Fallback –∫ –±–∞–∑–æ–≤–æ–º—É –∞–Ω–∞–ª–∏–∑—É

        return {
            'file_hash': file_hash,
            'file_size_bytes': file_size,
            'file_size_mb': file_size / (1024 * 1024),
            'estimated_pages': estimated_pages,
            'has_chinese_text': has_chinese_text,
            'recommended_ocr': not has_chinese_text,
            'processing_complexity': 'high' if file_size > 50*1024*1024 else 'medium'
        }

    except Exception as e:
        logger.warning(f"–ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ —É–¥–∞–ª—Å—è: {e}")
        return {
            'file_hash': 'unknown',
            'file_size_bytes': 0,
            'file_size_mb': 0.0,
            'estimated_pages': 1,
            'has_chinese_text': True,
            'recommended_ocr': True,
            'processing_complexity': 'medium'
        }

def process_document_with_api(**context) -> Dict[str, Any]:
    """‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ document-processor API —Å –≤—ã–±–æ—Ä–æ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ intermediate —Ñ–∞–π–ª–∞"""
    start_time = time.time()
    config = context['task_instance'].xcom_pull(task_ids='validate_input_file')

    try:
        input_file = config['input_file']
        timestamp = config['timestamp']
        filename = config['filename']

        logger.info(f"üîÑ –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ document-processor API: {DOCUMENT_PROCESSOR_URL}/process")

        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è (–ù–ï /app/temp!)
        airflow_temp = os.path.join(os.getenv('AIRFLOW_HOME', '/opt/airflow'), 'temp')
        os.makedirs(airflow_temp, exist_ok=True)

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ü–∏–π –¥–ª—è API
        use_ocr = bool(config.get("enable_ocr", False))
        recommended_ocr = config.get("chinese_doc_analysis", {}).get("recommended_ocr")
        if recommended_ocr and not use_ocr:
            logger.info(
                "üîï OCR recommendation ignored: digital PDF pipeline requires text-layer processing"
            )

        api_options = {
            "extract_tables": bool(config.get("extract_tables", True)),
            "extract_images": bool(config.get("extract_images", True)),
            "extract_formulas": bool(config.get("extract_formulas", True)),
            "use_ocr": use_ocr,
            "ocr_languages": config.get("ocr_languages", "eng,chi_sim"),
            "high_quality_ocr": bool(config.get("high_quality_ocr", True)),
            "preserve_layout": bool(config.get("preserve_structure", True)),
            "enable_chunking": False
        }

        def call_api(options: Dict[str, Any], attempt: int = 1) -> requests.Response:
            """–í—ã–∑–æ–≤ API document-processor —Å –æ–ø—Ü–∏—è–º–∏"""
            with open(input_file, 'rb') as f:
                files = {'file': (os.path.basename(input_file), f, 'application/pdf')}
                data = {'options': json.dumps(options, ensure_ascii=False)}

                logger.info(f"üì§ –ü–æ–ø—ã—Ç–∫–∞ {attempt}: –æ—Ç–ø—Ä–∞–≤–∫–∞ —Ñ–∞–π–ª–∞ {filename} —Å –æ–ø—Ü–∏—è–º–∏: {options}")

                return requests.post(
                    f"{DOCUMENT_PROCESSOR_URL}/process",
                    files=files,
                    data=data,
                    timeout=60*15  # 15 –º–∏–Ω—É—Ç —Ç–∞–π–º–∞—É—Ç
                )

        def perform_request(options: Dict[str, Any], attempt_start: int = 1) -> Tuple[requests.Response, Dict[str, Any], int]:
            """–í—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –±–µ–∑ —Ç–∞–±–ª–∏—Ü."""
            resp_local = call_api(options, attempt=attempt_start)
            effective_options = dict(options)
            next_attempt = attempt_start

            if (
                resp_local.status_code == 500
                and "TableData is not JSON serializable" in resp_local.text
                and effective_options.get("extract_tables", True)
            ):
                logger.warning(
                    "‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –æ—à–∏–±–∫–∞ —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ TableData. –ü–æ–≤—Ç–æ—Ä–Ω—ã–π –≤—ã–∑–æ–≤ –±–µ–∑ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü."
                )
                effective_options["extract_tables"] = False
                next_attempt += 1
                resp_local = call_api(effective_options, attempt=next_attempt)

            return resp_local, effective_options, next_attempt

        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–µ—Ä–≤—ã–π –≤—ã–∑–æ–≤ —Å –ø–æ–ª–Ω—ã–º–∏ –æ–ø—Ü–∏—è–º–∏
        resp, effective_api_options, last_attempt = perform_request(api_options, attempt_start=1)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–≤–µ—Ç–∞ API
        if resp.status_code != 200:
            error_msg = f"API –≤–µ—Ä–Ω—É–ª {resp.status_code}: {resp.text}"
            logger.error(f"‚ùå {error_msg}")
            return {"success": False, "error": error_msg, "original_config": config}

        # –ü–∞—Ä—Å–∏–Ω–≥ JSON –æ—Ç–≤–µ—Ç–∞
        try:
            resp_json = resp.json()
        except Exception as json_error:
            error_msg = f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π JSON –æ—Ç API: {json_error}"
            logger.error(f"‚ùå {error_msg}")
            return {"success": False, "error": error_msg, "original_config": config}

        if not resp_json.get("success", False):
            error_msg = f"API —Å–æ–æ–±—â–∏–ª –æ–± –æ—à–∏–±–∫–µ: {resp_json.get('message') or resp_json.get('error')}"
            logger.error(f"‚ùå {error_msg}")
            return {"success": False, "error": error_msg, "original_config": config}

        sections_count = int(resp_json.get("sections_count", 0) or 0)
        markdown_payload = (resp_json.get("markdown_content") or resp_json.get("raw_text") or "").strip()
        if sections_count == 0 and not markdown_payload:
            if not effective_api_options.get("use_ocr"):
                logger.warning(
                    "‚ö†Ô∏è Docling –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –±–µ–∑ OCR. –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ API —Å OCR –≤–∫–ª—é—á–µ–Ω–Ω—ã–º."
                )
                ocr_retry_options = {**effective_api_options, "use_ocr": True}
                resp, effective_api_options, last_attempt = perform_request(
                    ocr_retry_options, attempt_start=last_attempt + 1
                )

                if resp.status_code != 200:
                    error_msg = f"API –≤–µ—Ä–Ω—É–ª {resp.status_code}: {resp.text}"
                    logger.error(f"‚ùå {error_msg}")
                    return {"success": False, "error": error_msg, "original_config": config}

                try:
                    resp_json = resp.json()
                except Exception as json_error:
                    error_msg = f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π JSON –æ—Ç API (fallback OCR): {json_error}"
                    logger.error(f"‚ùå {error_msg}")
                    return {"success": False, "error": error_msg, "original_config": config}

                if not resp_json.get("success", False):
                    error_msg = (
                        "API —Å–æ–æ–±—â–∏–ª –æ–± –æ—à–∏–±–∫–µ –ø—Ä–∏ OCR fallback: "
                        f"{resp_json.get('message') or resp_json.get('error')}"
                    )
                    logger.error(f"‚ùå {error_msg}")
                    return {"success": False, "error": error_msg, "original_config": config}

                sections_count = int(resp_json.get("sections_count", 0) or 0)
                markdown_payload = (
                    (resp_json.get("markdown_content") or resp_json.get("raw_text") or "").strip()
                )

                if sections_count == 0 and not markdown_payload:
                    error_msg = (
                        "Docling –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–∂–µ –ø–æ—Å–ª–µ OCR fallback"
                    )
                    logger.error(f"‚ùå {error_msg}")
                    return {"success": False, "error": error_msg, "original_config": config}
            else:
                error_msg = (
                    "Docling –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É (sections=0, markdown missing); "
                    "pipeline –Ω–∞—Å—Ç—Ä–æ–µ–Ω –Ω–∞ —Ü–∏—Ñ—Ä–æ–≤—ã–µ PDF –±–µ–∑ OCR"
                )
                logger.error(f"‚ùå {error_msg}")
                return {"success": False, "error": error_msg, "original_config": config}

        # ‚úÖ –ù–û–í–û–ï: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –≤—ã–±–æ—Ä intermediate —Ñ–∞–π–ª–∞ –∏–∑ output_files
        def map_to_airflow_temp(path: str) -> str:
            """–ó–∞–º–µ–Ω–∞ –ø—Ä–µ—Ñ–∏–∫—Å–∞ /app/temp ‚Üí /opt/airflow/temp"""
            if path.startswith("/app/temp"):
                return path.replace("/app/temp", airflow_temp, 1)
            return path

        # 1) –ü—Ä–µ–¥–ø–æ—á–µ—Å—Ç—å doc_*_intermediate.json –∏–∑ output_files
        intermediate_file = resp_json.get("intermediate_file")
        output_files = resp_json.get("output_files", []) or []
        doc_intermediate = None

        for path in output_files:
            if isinstance(path, str) and path.endswith("_intermediate.json"):
                doc_intermediate = path
                break

        if doc_intermediate:
            mapped_path = map_to_airflow_temp(doc_intermediate)
            if os.path.exists(mapped_path):
                intermediate_file = mapped_path
                logger.info(f"‚úÖ –í—ã–±—Ä–∞–Ω –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π intermediate —Ñ–∞–π–ª: {intermediate_file}")

        # 2) –ï—Å–ª–∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –±–µ–∑–æ–ø–∞—Å–Ω—ã–π
        if not intermediate_file or not os.path.exists(intermediate_file):
            local_intermediate = os.path.join(airflow_temp, f"preprocessing_{timestamp}.json")
            safe_payload = {
                "title": resp_json.get("document_id", filename.replace('.pdf', '')),
                "pages_count": resp_json.get("pages_count", 0),
                "markdown_content": resp_json.get("markdown_content", ""),  # –≤–¥—Ä—É–≥ —Å–µ—Ä–≤–∏—Å –≤–µ—Ä–Ω—É–ª
                "raw_text": resp_json.get("raw_text", ""),
                "api_response": resp_json
            }

            with open(local_intermediate, "w", encoding="utf-8") as f:
                json.dump(safe_payload, f, ensure_ascii=False, indent=2)
            intermediate_file = local_intermediate
            logger.info(f"üìÅ –°–æ–∑–¥–∞–Ω –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ñ–∞–π–ª: {intermediate_file}")

        # ‚úÖ –ù–û–í–û–ï: 3) –°–æ—Ö—Ä–∞–Ω–∏—Ç—å bridge-—Ñ–∞–π–ª –¥–ª—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞
        bridge_file = os.path.join(airflow_temp, f"stage1_bridge_{timestamp}.json")
        try:
            with open(bridge_file, "w", encoding="utf-8") as f:
                json.dump({
                    "intermediate_file": intermediate_file,
                    "docling_intermediate": doc_intermediate,  # –∏—Å—Ö–æ–¥–Ω—ã–π –ø—É—Ç—å –æ—Ç API
                    "output_files": output_files
                }, f, ensure_ascii=False, indent=2)
            logger.info(f"üåâ –°–æ–∑–¥–∞–Ω bridge-—Ñ–∞–π–ª: {bridge_file}")
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–∞—Ç—å bridge-—Ñ–∞–π–ª Stage1: {e}")

        processing_time = resp_json.get("processing_time", time.time() - start_time)
        pages = resp_json.get("pages_count", 0)

        # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ OCR –Ω–∞ –æ—Å–Ω–æ–≤–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        use_ocr = bool(effective_api_options.get("use_ocr", False))
        config['enable_ocr'] = use_ocr

        result = {
            "success": True,
            "document_info": {
                "title": filename.replace('.pdf', ''),
                "total_pages": pages,
                "processing_time": processing_time,
                "status": "success"
            },
            "intermediate_file": intermediate_file,
            "original_config": config,
            "processing_stats": {
                "pages_processed": pages,
                "ocr_used": use_ocr,
                "processing_time_seconds": processing_time,
                "tables_extracted": effective_api_options.get("extract_tables", True),
                "api_attempts": last_attempt,
                "ocr_fallback_used": use_ocr and not api_options.get("use_ocr", False)
            },
            "api_response": resp_json,  # –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç API –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            "output_files": output_files  # ‚úÖ –ù–û–í–û–ï: –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–ª—è –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏
        }

        MetricsUtils.record_processing_metrics(
            dag_id='document_preprocessing',
            task_id='process_document_with_api',
            processing_time=processing_time,
            pages_count=pages,
            success=True
        )

        logger.info(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω —á–µ—Ä–µ–∑ API –∑–∞ {processing_time:.2f}—Å")
        return result

    except requests.RequestException as req_error:
        error_msg = f"–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ API: {req_error}"
        logger.error(f"‚ùå {error_msg}")
        MetricsUtils.record_processing_metrics(
            dag_id='document_preprocessing',
            task_id='process_document_with_api',
            processing_time=time.time() - start_time,
            success=False
        )
        return {"success": False, "error": error_msg, "original_config": config}

    except Exception as e:
        error_msg = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}"
        logger.error(f"‚ùå {error_msg}")
        MetricsUtils.record_processing_metrics(
            dag_id='document_preprocessing',
            task_id='process_document_with_api',
            processing_time=time.time() - start_time,
            success=False
        )
        return {"success": False, "error": error_msg, "original_config": config}

def prepare_for_next_stage(**context) -> Dict[str, Any]:
    """‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ DAG —Å –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–º –ø—É—Ç–µ–º"""
    start_time = time.time()
    try:
        result = context['task_instance'].xcom_pull(task_ids='process_document_with_api')
        if not result.get('success'):
            raise AirflowException(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: {result.get('error')}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        intermediate_file = result.get('intermediate_file')
        if not intermediate_file or not os.path.exists(intermediate_file):
            raise AirflowException("–ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ—Å–ª–µ Stage 1")

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ DAG
        next_stage_config = {
            'intermediate_file': intermediate_file,
            'original_config': result['original_config'],
            'dag1_metadata': {
                **result.get('processing_stats', {}),
                'completion_time': datetime.now().isoformat()
            },
            'dag1_completed': True,
            'ready_for_transformation': True,
            'chinese_document': True,
            'output_files': result.get('output_files', [])  # ‚úÖ –ù–û–í–û–ï: –ü–µ—Ä–µ–¥–∞–µ–º –¥–ª—è —Ñ–æ–ª–±—ç–∫–∞
        }

        # ‚úÖ –ù–û–í–û–ï: –¥—É–±–ª–∏—Ä—É–µ–º –≤ bridge –¥–ª—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞ (–Ω–∞ —Å–ª—É—á–∞–π –ø—Ä—è–º–æ–≥–æ —á—Ç–µ–Ω–∏—è)
        airflow_temp = os.path.join(os.getenv('AIRFLOW_HOME', '/opt/airflow'), 'temp')
        timestamp = result['original_config']['timestamp']
        bridge_file = os.path.join(airflow_temp, f"stage1_bridge_{timestamp}.json")
        try:
            with open(bridge_file, "w", encoding="utf-8") as f:
                json.dump({
                    "intermediate_file": intermediate_file,
                    "output_files": result.get('output_files', [])
                }, f, ensure_ascii=False, indent=2)
        except Exception:
            pass

        MetricsUtils.record_processing_metrics(
            dag_id='document_preprocessing',
            task_id='prepare_for_next_stage',
            processing_time=time.time() - start_time,
            success=True
        )

        logger.info("‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ DAG")
        return next_stage_config

    except Exception as e:
        MetricsUtils.record_processing_metrics(
            dag_id='document_preprocessing',
            task_id='prepare_for_next_stage',
            processing_time=time.time() - start_time,
            success=False
        )
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏: {e}")
        raise

def notify_completion(**context) -> None:
    """–£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏"""
    try:
        result = context['task_instance'].xcom_pull(task_ids='process_document_with_api')
        next_config = context['task_instance'].xcom_pull(task_ids='prepare_for_next_stage')

        if result and result.get('success'):
            stats = result.get('processing_stats', {})
            message = f"""
‚úÖ DOCUMENT PREPROCESSING –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û
üìÑ –§–∞–π–ª: {result['original_config']['filename']}
üìä –°—Ç—Ä–∞–Ω–∏—Ü: {stats.get('pages_processed', 'N/A')}
‚è±Ô∏è –í—Ä–µ–º—è: {stats.get('processing_time_seconds', 0):.2f}—Å
üîç OCR: {'–î–∞' if stats.get('ocr_used') else '–ù–µ—Ç'}
üìã –¢–∞–±–ª–∏—Ü—ã: {'–î–∞' if stats.get('tables_extracted') else '–ù–µ—Ç'}
üìÅ –§–∞–π–ª: {next_config.get('intermediate_file', 'N/A')}
‚úÖ –ì–æ—Ç–æ–≤ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç–∞–¥–∏–∏
"""
            NotificationUtils.send_success_notification(context, result)
        else:
            error = result.get('error', 'Unknown error') if result else 'No result'
            message = f"""
‚ùå DOCUMENT PREPROCESSING –ó–ê–í–ï–†–®–ï–ù –° –û–®–ò–ë–ö–û–ô
üìÑ –§–∞–π–ª: {result['original_config']['filename'] if result else 'Unknown'}
‚ùå –û—à–∏–±–∫–∞: {error}
‚è∞ –í—Ä–µ–º—è: {datetime.now().isoformat()}
"""
            NotificationUtils.send_failure_notification(context, Exception(error))

        logger.info(message)

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: {e}")

# ================================================================================
# –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ó–ê–î–ê–ß
# ================================================================================

validate_input = PythonOperator(
    task_id='validate_input_file',
    python_callable=validate_input_file,
    execution_timeout=timedelta(minutes=5),
    dag=dag
)

# ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: task_id –∏–∑–º–µ–Ω–µ–Ω –¥–ª—è —è—Å–Ω–æ—Å—Ç–∏ (—Ç–µ–ø–µ—Ä—å –≤—ã–∑—ã–≤–∞–µ—Ç API, –∞ –Ω–µ Docling –Ω–∞–ø—Ä—è–º—É—é)
process_document = PythonOperator(
    task_id='process_document_with_api',  # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–æ –¥–ª—è —è—Å–Ω–æ—Å—Ç–∏
    python_callable=process_document_with_api,
    execution_timeout=timedelta(hours=1),
    dag=dag
)

prepare_next = PythonOperator(
    task_id='prepare_for_next_stage',
    python_callable=prepare_for_next_stage,
    execution_timeout=timedelta(minutes=5),
    dag=dag
)

notify_task = PythonOperator(
    task_id='notify_completion',
    python_callable=notify_completion,
    trigger_rule='all_done',
    execution_timeout=timedelta(minutes=2),
    dag=dag
)

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
validate_input >> process_document >> prepare_next >> notify_task

def handle_processing_failure(context):
    """‚úÖ –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫"""
    try:
        failed_task = context['task_instance'].task_id
        exception = context.get('exception')

        error_message = f"""
üî• –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –í DOCUMENT PREPROCESSING v4.0
–ó–∞–¥–∞—á–∞: {failed_task}
–û—à–∏–±–∫–∞: {str(exception) if exception else 'Unknown'}
–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã –∏ —Ä–µ—à–µ–Ω–∏—è:
1. TableData —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è ‚Üí –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ —Å extract_tables=false
2. Permission denied ‚Üí –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é /opt/airflow/temp
3. API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Üí –ø—Ä–æ–≤–µ—Ä—å—Ç–µ document-processor:8001/health
4. –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–∞–π–ª ‚Üí –ø—Ä–æ–≤–µ—Ä—å—Ç–µ PDF –∏ –µ–≥–æ —Ä–∞–∑–º–µ—Ä
"""

        logger.error(error_message)
        ErrorHandlingUtils.handle_processing_error(context, exception, failed_task)

    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–µ: {e}")

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –æ—à–∏–±–æ–∫
for task in dag.tasks:
    task.on_failure_callback = handle_processing_failure
