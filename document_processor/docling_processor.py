#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
‚úÖ –ü–û–õ–ù–û–°–¢–¨–Æ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô DoclingProcessor
–†–µ—à–∞–µ—Ç –í–°–ï –ø—Ä–æ–±–ª–µ–º—ã —Å Docling API, OCR –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:
- ‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω –¥–æ –∞–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ Docling API v2.0+
- ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DocumentConverter
- ‚úÖ –£—Å–ª–æ–≤–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ OCR (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
- ‚úÖ –ö–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ pipeline_options
- ‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ Markdown
- ‚úÖ –£—Å—Ç—Ä–∞–Ω–µ–Ω—ã –ø–æ—Ç–µ—Ä–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
"""

import os
import io
import json
import logging
import asyncio
from typing import Dict, List, Optional, Any, Union
from pathlib import Path
import tempfile
from datetime import datetime
import traceback
import imghdr

# ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã Docling v2.0+
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling_core.types.doc import DoclingDocument
from docling.chunking import HybridChunker
from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
try:
    from transformers import AutoTokenizer
    HF_TRANSFORMERS_AVAILABLE = True
except ImportError:
    HF_TRANSFORMERS_AVAILABLE = False
    logging.warning("Transformers library not available, chunking will be limited")

from pydantic import BaseModel, Field
import structlog

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = structlog.get_logger("docling_processor")

def safe_serialize_tabledata(obj):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ TableData –∏ –¥—Ä—É–≥–∏—Ö Docling –æ–±—ä–µ–∫—Ç–æ–≤"""
    if hasattr(obj, '__dict__'):
        result = {'_type': obj.__class__.__name__}
        for key, value in obj.__dict__.items():
            if not key.startswith('_'):
                try:
                    import json
                    json.dumps(value)
                    result[key] = value
                except (TypeError, ValueError):
                    if hasattr(value, '__dict__'):
                        result[key] = safe_serialize_tabledata(value)
                    elif hasattr(value, '__iter__') and not isinstance(value, (str, bytes)):
                        result[key] = [safe_serialize_tabledata(item) for item in value]
                    else:
                        result[key] = str(value)
        return result
    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes)):
        return [safe_serialize_tabledata(item) for item in obj]
    else:
        try:
            import json
            json.dumps(obj)
            return obj
        except (TypeError, ValueError):
            return str(obj)

# ================================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–û–ù–ù–´–ï –ú–û–î–ï–õ–ò
# ================================================================================

class DoclingConfig(BaseModel):
    """‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è Docling –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞"""
    model_path: str = "/mnt/storage/models/docling"
    cache_dir: str = "/mnt/storage/models/docling"
    temp_dir: str = "/app/temp"
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    use_gpu: bool = True
    max_workers: int = 4
    enable_ocr_by_default: bool = False  # ‚úÖ –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é OCR –æ—Ç–∫–ª—é—á–µ–Ω
    
    # OCR –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (–∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è —É—Å–ª–æ–≤–Ω–æ)
    ocr_languages: List[str] = ["eng", "rus", "chi_sim"]
    ocr_confidence_threshold: float = 0.8
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    extract_tables: bool = True
    extract_images: bool = True
    extract_formulas: bool = True
    preserve_layout: bool = True
    
    # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    processing_timeout: int = 3600
    max_file_size_mb: int = 500
    
    class Config:
        env_prefix = "DOCLING_"

class DocumentStructure(BaseModel):
    """‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    title: str = ""
    authors: List[str] = []
    sections: List[Dict[str, Any]] = []
    tables: List[Dict[str, Any]] = []
    images: List[Dict[str, Any]] = []
    formulas: List[Dict[str, Any]] = []
    metadata: Dict[str, Any] = {}
    
    # ‚úÖ –ù–û–í–û–ï: –î–æ–±–∞–≤–ª–µ–Ω—ã –ø–æ–ª—è –¥–ª—è Markdown –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    raw_text: str = ""
    markdown_content: str = ""
    processing_stats: Dict[str, Any] = {}

# ================================================================================
# –û–°–ù–û–í–ù–û–ô DOCLING –ü–†–û–¶–ï–°–°–û–†
# ================================================================================

class DoclingProcessor:
    """‚úÖ –ü–û–õ–ù–û–°–¢–¨–Æ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Docling v2.0+"""
    
    def __init__(self, config: DoclingConfig):
        self.config = config
        self.converter: Optional[DocumentConverter] = None
        self.chunker: Optional[HybridChunker] = None
        self.ocr_initialized = False
        
        # –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        Path(self.config.cache_dir).mkdir(parents=True, exist_ok=True)
        Path(self.config.temp_dir).mkdir(parents=True, exist_ok=True)
        
        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–π –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä –ë–ï–ó OCR
        self._initialize_base_converter()

        # ‚úÖ –ò–ù–ò–¶–ò–ê–õ–ò–ó–ò–†–£–ï–ú CHUNKER, –ï–°–õ–ò –î–û–°–¢–£–ü–ù–´ –ù–ï–û–ë–•–û–î–ò–ú–´–ï –ó–ê–í–ò–°–ò–ú–û–°–¢–ò
        try:
            self._initialize_chunker()
        except Exception as chunker_error:
            logger.warning(
                "Chunker initialization failed during startup",
                error=str(chunker_error),
            )

        if self.chunker is not None:
            logger.info("Chunker status: enabled")
        else:
            status_msg = (
                "Chunker status: skipped (transformers not available)"
                if not HF_TRANSFORMERS_AVAILABLE
                else "Chunker status: disabled"
            )
            logger.info(status_msg)

        logger.info("DoclingProcessor initialized with conditional OCR loading")

    def _resolve_image_bytes(self, payload: Any) -> Optional[bytes]:
        """Attempt to extract raw bytes from a payload returned by Docling images."""

        if payload is None:
            return None

        try:
            if isinstance(payload, (bytes, bytearray, memoryview)):
                return bytes(payload)

            if isinstance(payload, (str, os.PathLike)):
                candidate_path = Path(payload)
                if candidate_path.exists():
                    try:
                        return candidate_path.read_bytes()
                    except Exception:
                        logger.debug("Failed to read image payload from path %s", candidate_path, exc_info=True)

            if isinstance(payload, io.BufferedIOBase):
                try:
                    current_position = None
                    if payload.seekable():
                        try:
                            current_position = payload.tell()
                        except Exception:
                            current_position = None
                        payload.seek(0)
                    data = payload.read()
                    if current_position is not None:
                        try:
                            payload.seek(current_position)
                        except Exception:
                            pass
                    if isinstance(data, str):
                        data = data.encode()
                    if isinstance(data, (bytes, bytearray, memoryview)):
                        return bytes(data)
                except Exception:
                    logger.debug("Buffered IO payload could not be read", exc_info=True)

            # io.BytesIO and similar expose getbuffer / getvalue / read methods
            if hasattr(payload, "getbuffer"):
                try:
                    buffer = payload.getbuffer()
                    return bytes(buffer)
                except Exception:
                    pass

            if hasattr(payload, "to_bytes") and callable(payload.to_bytes):
                try:
                    return payload.to_bytes()
                except Exception:
                    pass

            if hasattr(payload, "tobytes") and callable(payload.tobytes):
                try:
                    return payload.tobytes()
                except Exception:
                    pass

            if hasattr(payload, "getvalue") and callable(payload.getvalue):
                try:
                    value = payload.getvalue()
                    if isinstance(value, (bytes, bytearray, memoryview)):
                        return bytes(value)
                except Exception:
                    pass

            if hasattr(payload, "read") and callable(payload.read):
                try:
                    data = None
                    if hasattr(payload, "seek") and callable(payload.seek):
                        current_position = None
                        if hasattr(payload, "tell") and callable(payload.tell):
                            try:
                                current_position = payload.tell()
                            except Exception:
                                current_position = None
                        try:
                            payload.seek(0)
                        except Exception:
                            current_position = None
                        data = payload.read()
                        if current_position is not None:
                            try:
                                payload.seek(current_position)
                            except Exception:
                                pass
                    else:
                        data = payload.read()

                    if isinstance(data, str):
                        data = data.encode()

                    if isinstance(data, (bytes, bytearray, memoryview)):
                        return bytes(data)
                except Exception:
                    pass

        except Exception:
            logger.debug("Failed to resolve image payload into bytes", exc_info=True)

        return None

    def _detect_image_extension(self, image_obj: Any, image_bytes: Optional[bytes]) -> str:
        """Determine the most appropriate extension for an exported image."""

        format_hint: Optional[str] = None

        for attr in ("format", "mime_type", "media_type"):
            value = getattr(image_obj, attr, None)
            if value:
                format_hint = str(value).strip().lower()
                break

        extension = None
        if format_hint:
            if "/" in format_hint:
                format_hint = format_hint.split("/")[-1]
            format_hint = format_hint.lstrip(".")

            if format_hint == "jpeg":
                extension = "jpg"
            elif format_hint == "tiff":
                extension = "tif"
            elif format_hint:
                extension = format_hint

        if not extension and image_bytes:
            detected = imghdr.what(None, h=image_bytes)
            if detected == "jpeg":
                extension = "jpg"
            elif detected == "tiff":
                extension = "tif"
            elif detected:
                extension = detected

        return extension or "bin"

    def _initialize_base_converter(self):
        """‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑–æ–≤–æ–≥–æ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞ –±–µ–∑ OCR"""
        try:
            # –ë–∞–∑–æ–≤—ã–µ pipeline options –ë–ï–ó OCR
            base_pipeline_options = PdfPipelineOptions()
            base_pipeline_options.do_ocr = False  # ‚úÖ OCR –æ—Ç–∫–ª—é—á–µ–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            base_pipeline_options.do_table_structure = self.config.extract_tables
            base_pipeline_options.generate_page_images = self.config.extract_images

            # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ DocumentConverter
            self.converter = DocumentConverter(
                format_options={
                    InputFormat.PDF: PdfFormatOption(
                        pipeline_options=base_pipeline_options
                    )
                }
            )

            # ‚úÖ –í–ê–ñ–ù–û: –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ñ–ª–∞–≥ OCR, –∏–Ω–∞—á–µ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–ø—Ä–æ—Å—ã
            # –±–µ–∑ OCR –±—É–¥—É—Ç —Å—á–∏—Ç–∞—Ç—å, —á—Ç–æ –æ–Ω –µ—â—ë –∞–∫—Ç–∏–≤–µ–Ω.
            self.ocr_initialized = False

            logger.info("‚úÖ Base DocumentConverter initialized (OCR disabled)")

        except Exception as e:
            logger.error(f"‚ùå Failed to initialize base DocumentConverter: {e}")
            raise
    
    def _initialize_ocr_converter(self, ocr_languages: str = "eng"):
        """‚úÖ –ù–û–í–û–ï: –£—Å–ª–æ–≤–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞ —Å OCR"""
        try:
            # OCR pipeline options
            ocr_pipeline_options = PdfPipelineOptions()
            ocr_pipeline_options.do_ocr = True
            ocr_pipeline_options.do_table_structure = self.config.extract_tables
            ocr_pipeline_options.generate_page_images = self.config.extract_images
            
            # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞ —Å OCR
            self.converter = DocumentConverter(
                format_options={
                    InputFormat.PDF: PdfFormatOption(
                        pipeline_options=ocr_pipeline_options
                    )
                }
            )
            
            self.ocr_initialized = True
            logger.info(f"‚úÖ OCR DocumentConverter initialized with languages: {ocr_languages}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize OCR DocumentConverter: {e}")
            # Fallback –∫ –±–∞–∑–æ–≤–æ–º—É –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä—É
            self._initialize_base_converter()
            raise
    
    def _initialize_chunker(self):
        """‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è chunker –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        if not HF_TRANSFORMERS_AVAILABLE:
            logger.warning("Transformers not available, skipping chunker initialization")
            return
            
        try:
            # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
            tokenizer = HuggingFaceTokenizer(
                tokenizer=AutoTokenizer.from_pretrained(
                    "sentence-transformers/all-MiniLM-L6-v2"
                )
            )
            
            self.chunker = HybridChunker(
                tokenizer=tokenizer,
                max_tokens=1024,
                overlap_tokens=128
            )
            
            logger.info("‚úÖ HybridChunker initialized")
            
        except Exception as e:
            logger.warning(f"Failed to initialize chunker: {e}")
            self.chunker = None
    
    async def process_document(
        self,
        file_path: str,
        output_dir: str,
        use_ocr: bool = False,
        ocr_languages: str = "eng",
        allow_ocr_fallback: bool = True,
    ) -> DocumentStructure:
        """
        ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        
        Args:
            file_path: –ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            use_ocr: –í–∫–ª—é—á–∏—Ç—å –ª–∏ OCR –æ–±—Ä–∞–±–æ—Ç–∫—É
            ocr_languages: –Ø–∑—ã–∫–∏ –¥–ª—è OCR (eng, rus, chi_sim)
            
        Returns:
            DocumentStructure: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        """
        start_time = datetime.now()
        
        try:
            logger.info(f"üîÑ Starting document processing: {file_path}")
            logger.info(f"   OCR enabled: {use_ocr}")
            logger.info(f"   OCR languages: {ocr_languages}")

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"File not found: {file_path}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
            file_size = os.path.getsize(file_path)
            max_size = self.config.max_file_size_mb * 1024 * 1024
            if file_size > max_size:
                raise ValueError(f"File too large: {file_size} bytes (max: {max_size})")

            # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –£—Å–ª–æ–≤–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OCR
            if use_ocr and not self.ocr_initialized:
                logger.info("üîÑ Initializing OCR converter...")
                self._initialize_ocr_converter(ocr_languages)
            elif not use_ocr and self.ocr_initialized:
                logger.info("üîÑ Switching back to base converter...")
                self._initialize_base_converter()

            fallback_used = False

            # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º API
            logger.info("üîÑ Converting document with Docling...")
            conversion_result = self.converter.convert(file_path)
            docling_document = conversion_result.document

            # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Docling –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document_structure = await self._extract_document_structure(
                docling_document, file_path, output_dir
            )

            def _structure_is_empty(struct: DocumentStructure) -> bool:
                return (
                    len(struct.sections) == 0
                    and not (struct.markdown_content or "").strip()
                    and not (struct.raw_text or "").strip()
                )

            # ‚úÖ –ù–û–í–û–ï: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π fallback –Ω–∞ OCR, –µ—Å–ª–∏ –±–µ–∑ OCR –∫–æ–Ω—Ç–µ–Ω—Ç –ø—É—Å—Ç–æ–π
            if (
                _structure_is_empty(document_structure)
                and not use_ocr
                and allow_ocr_fallback
            ):
                logger.warning(
                    "‚ö†Ô∏è Docling –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –±–µ–∑ OCR. –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —Å OCR –≤–∫–ª—é—á–µ–Ω–Ω—ã–º."
                )
                try:
                    self._initialize_ocr_converter(ocr_languages)
                    conversion_result = self.converter.convert(file_path)
                    docling_document = conversion_result.document
                    document_structure = await self._extract_document_structure(
                        docling_document, file_path, output_dir
                    )
                    use_ocr = True
                    fallback_used = True
                except Exception as fallback_err:
                    logger.error(
                        "‚ùå OCR fallback failed",
                        error=str(fallback_err),
                    )
                    raise

            if _structure_is_empty(document_structure):
                if allow_ocr_fallback:
                    raise ValueError(
                        "Docling returned empty structure even after OCR fallback"
                    )
                logger.warning(
                    "‚ö†Ô∏è Docling –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É, OCR fallback –æ—Ç–∫–ª—é—á–µ–Ω."
                )
                raise ValueError(
                    "Docling returned empty structure and OCR fallback is disabled"
                )

            # –†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            processing_time = (datetime.now() - start_time).total_seconds()
            document_structure.processing_stats = {
                "processing_time_seconds": processing_time,
                "file_size_bytes": file_size,
                "ocr_used": use_ocr,
                "ocr_languages": ocr_languages if use_ocr else None,
                "docling_version": "2.0+",
                "timestamp": datetime.now().isoformat(),
                "ocr_fallback_triggered": fallback_used,
                "ocr_fallback_allowed": allow_ocr_fallback,
            }

            document_structure.metadata.setdefault("processing_notes", [])
            if fallback_used:
                document_structure.metadata["processing_notes"].append(
                    "ocr_fallback_applied"
                )
            document_structure.metadata["has_ocr_content"] = bool(use_ocr)

            logger.info(f"‚úÖ Document processing completed in {processing_time:.2f}s")
            return document_structure

        except Exception as e:
            logger.error(f"‚ùå Error processing document: {e}")
            logger.error(traceback.format_exc())
            raise
    
    async def _extract_document_structure(
        self, 
        docling_document: DoclingDocument, 
        original_file_path: str,
        output_dir: str
    ) -> DocumentStructure:
        """‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏–∑ Docling –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        try:
            # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            title = getattr(docling_document, 'title', '') or \
                   Path(original_file_path).stem
            
            # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç –≤ Markdown —Å —Ñ–æ–ª–±—ç–∫–æ–º
            markdown_content = ""
            try:
                markdown_content = docling_document.export_to_markdown() or ""
            except Exception as md_error:
                logger.warning(f"Markdown export failed, fallback to structural reconstruction: {md_error}")
                markdown_content = ""

            if not isinstance(markdown_content, str):
                markdown_content = str(markdown_content)

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            raw_text = getattr(docling_document, 'text', '') or ""
            if not isinstance(raw_text, str):
                raw_text = str(raw_text)

            # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü –∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            total_pages = len(docling_document.pages) if hasattr(docling_document, 'pages') else 1

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–∫—Ü–∏–π
            sections = []
            if hasattr(docling_document, 'sections'):
                for i, section in enumerate(docling_document.sections):
                    sections.append({
                        "id": i,
                        "title": getattr(section, 'title', f'Section {i+1}'),
                        "content": getattr(section, 'text', ''),
                        "level": getattr(section, 'level', 1),
                        "page": getattr(section, 'page', 1)
                    })

            for section in sections:
                section['title'] = str(section.get('title') or f"Section {section.get('id', 0) + 1}").strip()
                section['content'] = str(section.get('content') or "").strip()
                section['level'] = int(section.get('level') or 1)
                section['page'] = int(section.get('page') or 1)

            generated_outline = False
            if not sections and markdown_content:
                sections = self._build_sections_from_markdown(markdown_content)
                generated_outline = True

            if not raw_text:
                raw_text = self._markdown_to_plain_text(markdown_content) if markdown_content else ""

            if markdown_content == "" and raw_text:
                markdown_content = self._create_markdown_from_plain_text(raw_text)

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü
            tables = []
            if hasattr(docling_document, 'tables'):
                for i, table in enumerate(docling_document.tables):
                    table_data = {
                        "id": i,
                        "page": getattr(table, 'page', 1),
                        "content": self._extract_table_content(table),
                        "bbox": getattr(table, 'bbox', None)
                    }
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª
                    table_file = Path(output_dir) / f"table_{i}.json"
                    with open(table_file, 'w', encoding='utf-8') as f:
                        json.dump(table_data, f, ensure_ascii=False, indent=2, default=safe_serialize_tabledata)
                    
                    table_data["file_path"] = str(table_file)
                    tables.append(table_data)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            images = []
            output_dir_path = Path(output_dir)
            output_dir_path.mkdir(parents=True, exist_ok=True)

            if hasattr(docling_document, 'images'):
                for i, image in enumerate(docling_document.images):
                    image_data = {
                        "id": i,
                        "page": getattr(image, 'page', 1),
                        "bbox": getattr(image, 'bbox', None),
                        "format": getattr(image, 'format', 'unknown')
                    }

                    payload_candidates: List[Any] = []

                    for attr_name in ("data", "content", "buffer", "bytes", "raw"):
                        if not hasattr(image, attr_name):
                            continue
                        candidate = getattr(image, attr_name)
                        if callable(candidate):
                            try:
                                candidate = candidate()
                            except Exception:
                                continue
                        payload_candidates.append(candidate)

                    for method_name in ("get_data", "getbuffer", "to_bytes", "tobytes"):
                        method = getattr(image, method_name, None)
                        if callable(method):
                            try:
                                payload_candidates.append(method())
                            except Exception:
                                continue

                    saved = False
                    for payload in payload_candidates:
                        image_bytes = self._resolve_image_bytes(payload)
                        if not image_bytes:
                            continue

                        extension = self._detect_image_extension(image, image_bytes)
                        image_file = output_dir_path / f"image_{i}.{extension}"

                        try:
                            with open(image_file, "wb") as f:
                                f.write(image_bytes)
                            image_data["file_path"] = str(image_file.resolve())
                            image_data["size_bytes"] = len(image_bytes)
                            image_data["extension"] = extension
                            saved = True
                            break
                        except Exception as write_error:
                            logger.warning(
                                "Failed to write image %s to %s: %s",
                                i,
                                image_file,
                                write_error,
                            )
                            if image_file.exists():
                                try:
                                    image_file.unlink()
                                except Exception:
                                    logger.debug(
                                        "Failed to clean up incomplete image file %s",
                                        image_file,
                                        exc_info=True,
                                    )

                    if not saved:
                        logger.warning(
                            "Unable to persist image %s from %s", i, original_file_path
                        )
                        image_data.setdefault("errors", []).append("persist_failed")
                        image_data["file_path"] = None

                    images.append(image_data)
            
            # ‚úÖ –ù–û–í–û–ï: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ chunking –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
            chunks = []
            if self.chunker and markdown_content:
                try:
                    chunks = list(self.chunker.chunk(docling_document))
                    logger.info(f"üìù Document chunked into {len(chunks)} pieces")
                except Exception as e:
                    logger.warning(f"Chunking failed: {e}")
            
            # –°–æ–∑–¥–∞–Ω–∏–µ DocumentStructure
            document_structure = DocumentStructure(
                title=title,
                authors=[],  # TODO: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–æ–≤ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
                sections=sections,
                tables=tables,
                images=images,
                formulas=[],  # TODO: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–æ—Ä–º—É–ª
                raw_text=raw_text,
                markdown_content=markdown_content,
                metadata={
                    "original_file": original_file_path,
                    "total_pages": total_pages,
                    "sections_count": len(sections),
                    "tables_count": len(tables),
                    "images_count": len(images),
                    "chunks_count": len(chunks),
                    "has_ocr_content": self.ocr_initialized,
                    "extraction_method": "docling_v2",
                    "content_length": len(raw_text),
                    "section_titles": [section.get("title") for section in sections],
                    "outline_generated": generated_outline
                }
            )

            logger.info(f"‚úÖ Document structure extracted: {len(sections)} sections, "
                       f"{len(tables)} tables, {len(images)} images")
            
            return document_structure
            
        except Exception as e:
            logger.error(f"‚ùå Error extracting document structure: {e}")
            logger.error(traceback.format_exc())
            raise

    def _extract_table_content(self, table) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ç–∞–±–ª–∏—Ü—ã"""
        try:
            if hasattr(table, 'data'):
                return {
                    "type": "table",
                    "data": table.data,
                    "rows": getattr(table, 'rows', 0),
                    "columns": getattr(table, 'columns', 0)
                }
            else:
                return {
                    "type": "table",
                    "content": str(table),
                    "extracted_method": "string_representation"
                }
        except Exception as e:
            logger.warning(f"Failed to extract table content: {e}")
            return {"type": "table", "error": str(e)}

    def _build_sections_from_markdown(self, markdown_content: str) -> List[Dict[str, Any]]:
        """–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–µ–∫—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ Markdown —Ä–∞–∑–º–µ—Ç–∫–∏"""
        sections: List[Dict[str, Any]] = []
        current: Optional[Dict[str, Any]] = None

        for line in markdown_content.splitlines():
            stripped = line.strip()

            if stripped.startswith('#'):
                level = len(stripped) - len(stripped.lstrip('#'))
                title = stripped[level:].strip() or f"Section {len(sections) + 1}"

                if current is not None:
                    current['content'] = "\n".join(current.pop('content_lines')).strip()
                    sections.append(current)

                current = {
                    'id': len(sections),
                    'title': title,
                    'level': max(1, min(level, 6)),
                    'page': 1,
                    'content_lines': []
                }
            else:
                if current is None:
                    current = {
                        'id': len(sections),
                        'title': f"Section {len(sections) + 1}",
                        'level': 1,
                        'page': 1,
                        'content_lines': []
                    }
                current['content_lines'].append(line)

        if current is not None:
            current['content'] = "\n".join(current.pop('content_lines')).strip()
            sections.append(current)

        if not sections and markdown_content.strip():
            sections.append({
                'id': 0,
                'title': 'Document',
                'level': 1,
                'page': 1,
                'content': markdown_content.strip()
            })
        else:
            for idx, section in enumerate(sections):
                section.setdefault('content', '')
                section['content'] = section['content'].strip()
                section['id'] = idx
                section.setdefault('page', 1)

        return sections

    def _markdown_to_plain_text(self, markdown_content: str) -> str:
        """–ü—Ä–æ—Å—Ç–µ–π—à–µ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ Markdown –≤ —Ç–µ–∫—Å—Ç"""
        plain_lines: List[str] = []
        for line in markdown_content.splitlines():
            stripped = line.strip()
            if not stripped:
                plain_lines.append("")
                continue

            if stripped.startswith('#'):
                stripped = stripped.lstrip('#').strip()

            if stripped.startswith('|') and stripped.endswith('|'):
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏-—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ —Ç–∞–±–ª–∏—Ü
                continue
            if set(stripped) <= {'|', ':', '-', ' '}:
                continue

            plain_lines.append(stripped.replace('`', ''))

        return "\n".join(plain_lines).strip()

    def _create_markdown_from_plain_text(self, plain_text: str) -> str:
        """–§–æ–ª–±—ç–∫: —Å–æ–∑–¥–∞–µ–º Markdown –∏–∑ –ø—Ä–æ—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        return plain_text.strip()

    def export_to_markdown(
        self, 
        document_structure: DocumentStructure, 
        output_file: str
    ) -> str:
        """‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –≠–∫—Å–ø–æ—Ä—Ç –≤ Markdown —Ñ–∞–π–ª"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –≥–æ—Ç–æ–≤—ã–π markdown –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Ç Docling
            markdown_content = document_structure.markdown_content
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if not markdown_content:
                # Fallback: —Å–æ–∑–¥–∞–µ–º markdown –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                markdown_content = self._create_markdown_from_structure(document_structure)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            logger.info(f"‚úÖ Markdown exported to: {output_file}")
            return markdown_content
            
        except Exception as e:
            logger.error(f"‚ùå Error exporting to markdown: {e}")
            raise
    
    def _create_markdown_from_structure(self, document_structure: DocumentStructure) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ Markdown –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ (fallback)"""
        lines = []
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        if document_structure.title:
            lines.append(f"# {document_structure.title}\n")
        
        # –ê–≤—Ç–æ—Ä—ã
        if document_structure.authors:
            lines.append(f"**Authors:** {', '.join(document_structure.authors)}\n")
        
        # –°–µ–∫—Ü–∏–∏
        for section in document_structure.sections:
            level = "#" * min(section.get("level", 1) + 1, 6)
            lines.append(f"{level} {section.get('title', 'Untitled Section')}\n")
            lines.append(f"{section.get('content', '')}\n")
        
        # –¢–∞–±–ª–∏—Ü—ã
        if document_structure.tables:
            lines.append("## Tables\n")
            for i, table in enumerate(document_structure.tables):
                lines.append(f"### Table {i+1}\n")
                lines.append(f"Page: {table.get('page', 'N/A')}\n")
                if table.get('file_path'):
                    lines.append(f"Data file: {table['file_path']}\n")
        
        return "\n".join(lines)
    
    def cleanup_temp_files(self, output_dir: str, keep_main_files: bool = True):
        """–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        try:
            output_path = Path(output_dir)
            if output_path.exists():
                for file_path in output_path.glob("*"):
                    if keep_main_files and file_path.suffix in ['.md', '.json']:
                        continue
                    file_path.unlink()
                logger.info(f"üßπ Cleaned up temporary files in {output_dir}")
        except Exception as e:
            logger.warning(f"Failed to cleanup temp files: {e}")
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞"""
        return {
            "converter_initialized": self.converter is not None,
            "ocr_initialized": self.ocr_initialized,
            "chunker_available": self.chunker is not None,
            "config": self.config.dict(),
            "transformers_available": HF_TRANSFORMERS_AVAILABLE
        }

# ================================================================================
# –£–¢–ò–õ–ò–¢–´ –ò –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ================================================================================

def create_docling_processor_from_env() -> DoclingProcessor:
    """‚úÖ –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
    config = DoclingConfig(
        model_path=os.getenv('DOCLING_MODEL_PATH', '/mnt/storage/models/docling'),
        cache_dir=os.getenv('DOCLING_HOME', '/mnt/storage/models/docling'),
        temp_dir=os.getenv('TEMP_DIR', '/app/temp'),
        use_gpu=os.getenv('DOCLING_USE_GPU', 'true').lower() == 'true',
        max_workers=int(os.getenv('DOCLING_MAX_WORKERS', '4')),
        enable_ocr_by_default=os.getenv('DEFAULT_USE_OCR', 'false').lower() == 'true',
        ocr_confidence_threshold=float(os.getenv('OCR_CONFIDENCE_THRESHOLD', '0.8')),
        extract_tables=os.getenv('DEFAULT_EXTRACT_TABLES', 'true').lower() == 'true',
        extract_images=os.getenv('DEFAULT_EXTRACT_IMAGES', 'true').lower() == 'true',
        processing_timeout=int(os.getenv('PROCESSING_TIMEOUT_MINUTES', '60')) * 60,
        max_file_size_mb=int(os.getenv('MAX_FILE_SIZE_MB', '500'))
    )
    
    return DoclingProcessor(config)

# ================================================================================
# –≠–ö–°–ü–û–†–¢
# ================================================================================

__all__ = [
    'DoclingProcessor',
    'DoclingConfig', 
    'DocumentStructure',
    'create_docling_processor_from_env'
]
