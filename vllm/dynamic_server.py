#!/usr/bin/env python3
"""
–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π vLLM —Å–µ—Ä–≤–µ—Ä —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–º–µ–Ω–æ–π –º–æ–¥–µ–ª–µ–π
PDF Converter Pipeline v2.0 - –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
"""

import asyncio
import uvicorn
import logging
import os
import time
from contextlib import asynccontextmanager
from typing import List, Optional, Dict, Any

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

from model_manager import model_manager, TaskType, initialize_model_manager

MAX_CONCURRENT_REQUESTS = max(1, int(os.getenv('VLLM_MAX_CONCURRENT', '2')))
QUEUE_TIMEOUT_SECONDS = float(os.getenv('VLLM_QUEUE_TIMEOUT_SECONDS', '10'))
MAX_ALLOWED_TOKENS = int(os.getenv('VLLM_MAX_TOKENS', '4096'))
MAX_ALLOWED_TEMPERATURE = float(os.getenv('VLLM_MAX_TEMPERATURE', '0.8'))

_REQUEST_SEMAPHORE = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Pydantic –º–æ–¥–µ–ª–∏
class ChatMessage(BaseModel):
    role: str = Field(..., description="–†–æ–ª—å: system, user, assistant")
    content: str = Field(..., description="–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è")

class ChatCompletionRequest(BaseModel):
    model: str = Field(..., description="–ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏")
    messages: List[ChatMessage] = Field(..., description="–°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π")
    temperature: float = Field(0.1, ge=0.0, le=2.0)
    max_tokens: int = Field(4096, ge=1, le=32768)
    top_p: float = Field(0.9, ge=0.0, le=1.0)
    top_k: int = Field(50, ge=1, le=100)
    stream: bool = Field(False)
    task_type: Optional[str] = Field(None, description="–¢–∏–ø –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏")

class ModelSwapRequest(BaseModel):
    model_key: str = Field(..., description="–ö–ª—é—á –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")

# Lifespan
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ Dynamic vLLM Server –¥–ª—è PDF Converter Pipeline v2.0")
    success = await initialize_model_manager()
    if not success:
        logger.error("‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Model Manager")
        raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Model Manager")
    logger.info("‚úÖ Dynamic vLLM Server –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
    yield
    # Shutdown
    logger.info("üîÑ –û—Å—Ç–∞–Ω–æ–≤–∫–∞ Dynamic vLLM Server")
    if model_manager.current_model:
        await model_manager.unload_current_model()
    logger.info("‚úÖ Dynamic vLLM Server –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

# FastAPI app
app = FastAPI(
    title="Dynamic vLLM Server",
    description="vLLM —Å–µ—Ä–≤–µ—Ä —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –ø–æ–¥–≥—Ä—É–∑–∫–æ–π –º–æ–¥–µ–ª–µ–π",
    version="2.0",
    lifespan=lifespan,
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def determine_task_type_from_messages(messages: List[ChatMessage]) -> TaskType:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏ –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π"""
    try:
        combined_text = " ".join([msg.content.lower() for msg in messages])
        content_keywords = [
            "–ø—Ä–µ–æ–±—Ä–∞–∑—É–π", "markdown", "—Å—Ç—Ä—É–∫—Ç—É—Ä–∞", "—Ç–∞–±–ª–∏—Ü–∞",
            "pdf", "–¥–æ–∫—É–º–µ–Ω—Ç", "–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ", "—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ",
        ]
        translation_keywords = [
            "–ø–µ—Ä–µ–≤–µ–¥–∏", "translate", "–ø–µ—Ä–µ–≤–æ–¥", "translation",
            "—Ä—É—Å—Å–∫–∏–π", "english", "‰∏≠Êñá", "—è–∑—ã–∫", "language",
        ]
        content_score = sum(1 for keyword in content_keywords if keyword in combined_text)
        translation_score = sum(1 for keyword in translation_keywords if keyword in combined_text)
        return TaskType.TRANSLATION if translation_score > content_score else TaskType.CONTENT_TRANSFORMATION
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏: {e}")
        return TaskType.CONTENT_TRANSFORMATION

@app.post("/v1/chat/completions")
async def create_chat_completion(request: ChatCompletionRequest):
    """OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π endpoint —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–º–µ–Ω–æ–π –º–æ–¥–µ–ª–µ–π"""
    try:
        start_time = time.time()

        if request.stream:
            raise HTTPException(status_code=400, detail="Streaming responses –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è")

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
        task_type: Optional[TaskType] = None
        if request.task_type:
            try:
                task_type = TaskType(request.task_type)
            except ValueError:
                task_type = None
        if not task_type:
            task_type = determine_task_type_from_messages(request.messages)
        logger.info(f"üìù –ó–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏. –¢–∏–ø –∑–∞–¥–∞—á–∏: {task_type.value}")

        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –Ω—É–∂–Ω–æ–π –º–æ–¥–µ–ª–∏
        model_ready = await model_manager.ensure_model_loaded(task_type)
        if not model_ready:
            raise HTTPException(
                status_code=503,
                detail=f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–¥–∞—á–∏ {task_type.value}",
            )

        queue_acquired = False
        queue_start = time.time()
        try:
            await asyncio.wait_for(_REQUEST_SEMAPHORE.acquire(), timeout=QUEUE_TIMEOUT_SECONDS)
            queue_acquired = True
        except asyncio.TimeoutError:
            raise HTTPException(status_code=503, detail="vLLM –∑–∞–Ω—è—Ç, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ")

        queue_wait = time.time() - queue_start
        concurrency_in_use = MAX_CONCURRENT_REQUESTS - _REQUEST_SEMAPHORE._value

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ vLLM engine
        if not model_manager.vllm_engine:
            raise HTTPException(status_code=503, detail="vLLM engine –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ (—Ñ–æ—Ä–º–∞—Ç Qwen Chat)
        prompt_parts: List[str] = []
        for message in request.messages:
            if message.role == "system":
                prompt_parts.append(f"<|im_start|>system\n{message.content}<|im_end|>")
            elif message.role == "user":
                prompt_parts.append(f"<|im_start|>user\n{message.content}<|im_end|>")
            elif message.role == "assistant":
                prompt_parts.append(f"<|im_start|>assistant\n{message.content}<|im_end|>")
        prompt_parts.append("<|im_start|>assistant\n")
        formatted_prompt = "\n".join(prompt_parts)

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        from vllm import SamplingParams
        from vllm.utils import random_uuid

        effective_max_tokens = max(1, min(request.max_tokens, MAX_ALLOWED_TOKENS))
        effective_temperature = max(0.0, min(request.temperature, MAX_ALLOWED_TEMPERATURE))

        sampling_params = SamplingParams(
            temperature=effective_temperature,
            max_tokens=effective_max_tokens,
            top_p=request.top_p,
            top_k=request.top_k,
            stop=["<|im_end|>"],
        )

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        request_id = random_uuid()
        results = model_manager.vllm_engine.generate(
            formatted_prompt,
            sampling_params,
            request_id=request_id,
        )

        # –û–∂–∏–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        final_output = None
        async for request_output in results:
            final_output = request_output

        if final_output is None or not final_output.outputs:
            raise HTTPException(status_code=500, detail="No output generated")

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –æ–±—Ä–∞—â–µ–Ω–∏–µ –∫ –ø–æ–ª—è–º vLLM RequestOutput
        generated_text = (final_output.outputs[0].text or "").strip()
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: prompt_token_ids - —ç—Ç–æ —Å–ø–∏—Å–æ–∫, –Ω–µ –º–∞—Å—Å–∏–≤ –æ–±—ä–µ–∫—Ç–æ–≤
        prompt_tokens = len(final_output.prompt_token_ids or [])
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ token_ids –∏–∑ CompletionOutput
        try:
            completion_tokens = len(final_output.outputs[0].token_ids or [])
        except (AttributeError, TypeError):
            # Fallback –µ—Å–ª–∏ token_ids –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
            completion_tokens = max(1, len(generated_text.split()) // 4)  # –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            
        total_tokens = prompt_tokens + completion_tokens
        processing_time = time.time() - start_time

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        response = {
            "id": f"chatcmpl-{request_id}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model_manager.models[model_manager.current_model].name,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": generated_text
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": total_tokens
            },
            "pdf_converter_meta": {
                "task_type": task_type.value,
                "model_key": model_manager.current_model,
                "processing_time_seconds": round(processing_time, 2),
                "vram_usage_gb": round(max(0.0, 48.0 - model_manager.get_available_vram_gb()), 1),
                "queue_wait_seconds": round(queue_wait, 3),
                "concurrency_in_use": concurrency_in_use,
                "max_concurrent": MAX_CONCURRENT_REQUESTS,
                "max_tokens_effective": effective_max_tokens,
            }
        }

        logger.info(
            "‚úÖ –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∑–∞ %.2fs (–∂–¥–∞–ª–∏ %.2fs). –¢–æ–∫–µ–Ω–æ–≤: %d",
            processing_time,
            queue_wait,
            total_tokens,
        )
        return response

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}")
    finally:
        if '_REQUEST_SEMAPHORE' in globals():
            # –æ—Å–≤–æ–±–æ–∂–¥–∞–µ–º —Å–µ–º–∞—Ñ–æ—Ä —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω –±—ã–ª –∑–∞—Ö–≤–∞—á–µ–Ω
            if 'queue_acquired' in locals() and queue_acquired:
                _REQUEST_SEMAPHORE.release()

@app.post("/v1/models/swap")
async def swap_model(request: ModelSwapRequest):
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–º–µ–Ω–∞ –º–æ–¥–µ–ª–∏"""
    try:
        logger.info(f"üîÑ –ó–∞–ø—Ä–æ—Å —Å–º–µ–Ω—ã –º–æ–¥–µ–ª–∏ –Ω–∞: {request.model_key}")
        if request.model_key not in model_manager.models:
            available_models = list(model_manager.models.keys())
            raise HTTPException(
                status_code=400,
                detail=f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å: {request.model_key}. –î–æ—Å—Ç—É–ø–Ω—ã: {available_models}",
            )
        success = await model_manager.load_model(request.model_key)
        if success:
            return {
                "status": "success",
                "message": f"–ú–æ–¥–µ–ª—å {request.model_key} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞",
                "current_model": model_manager.current_model,
                "vram_available": model_manager.get_available_vram_gb(),
            }
        else:
            raise HTTPException(
                status_code=503,
                detail=f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {request.model_key}",
            )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–º–µ–Ω—ã –º–æ–¥–µ–ª–∏: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/v1/models")
async def list_models():
    """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    models_info: Dict[str, Dict[str, Any]] = {}
    for model_key, config in model_manager.models.items():
        models_info[model_key] = {
            "name": config.name,
            "alias": config.alias,
            "task_type": config.task_type.value,
            "state": model_manager.model_states[model_key].value,
        }

    models_list: List[Dict[str, Any]] = []
    for model_key, info in models_info.items():
        models_list.append({
            "id": info["name"],
            "object": "model",
            "created": int(time.time()),
            "owned_by": "pdf-converter-v2",
            "pdf_converter_meta": {
                "key": model_key,
                "alias": info["alias"],
                "task_type": info["task_type"],
                "state": info["state"],
            },
        })
    return {"object": "list", "data": models_list}

@app.get("/v1/models/status")
async def models_status():
    """–î–µ—Ç–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã –º–æ–¥–µ–ª–µ–π"""
    return {
        "manager_status": model_manager.get_status(),
        "system_info": {
            "total_vram_gb": 96.0,  # 2x A6000
            "available_vram_gb": model_manager.get_available_vram_gb(),
            "gpu_count": 2,
            "gpu_type": "NVIDIA A6000",
        },
    }

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–µ—Ä–∞"""
    status = model_manager.get_status()
    is_healthy = (
        model_manager.current_model is not None and
        status["available_vram_gb"] > 2.0
    )
    response = {
        "status": "healthy" if is_healthy else "unhealthy",
        "timestamp": time.time(),
        "current_model": status["current_model"],
        "available_vram_gb": status["available_vram_gb"],
        "version": "v2.0-dynamic",
    }
    return JSONResponse(
        content=response,
        status_code=200 if is_healthy else 503,
    )

@app.get("/metrics")
async def metrics():
    """–ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è Prometheus"""
    status = model_manager.get_status()
    metrics_lines = [
        "# HELP vllm_available_vram_gb Available VRAM in GB",
        "# TYPE vllm_available_vram_gb gauge",
        f'vllm_available_vram_gb {status["available_vram_gb"]}',
        "",
        "# HELP vllm_model_loaded Current model loaded (1=loaded, 0=not loaded)",
        "# TYPE vllm_model_loaded gauge",
    ]
    for model_key, config in model_manager.models.items():
        is_loaded = 1 if model_manager.model_states[model_key].name == "LOADED" else 0
        metrics_lines.append(
            f'vllm_model_loaded{{model_key="{model_key}",model_name="{config.name}"}} {is_loaded}'
        )
    return Response(
        content="\n".join(metrics_lines),
        media_type="text/plain",
    )

if __name__ == "__main__":
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "8000"))
    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ Dynamic vLLM Server –Ω–∞ {host}:{port}")
    uvicorn.run(
        app,
        host=host,
        port=port,
        log_level="info",
        access_log=True,
        workers=1,  # –≤–∞–∂–Ω–æ: 1 worker –¥–ª—è GPU
        reload=False,
    )
